{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tolerevo/00-hola-mundo/blob/main/Resume_AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kazd7ZJln6gO"
      },
      "source": [
        "# How to Use this Notebook\n",
        "\n",
        "If you want to play around with the code, you'll need to hit **file** > **save a copy in Drive** and then work in that copy.\n",
        "\n",
        "---\n",
        "\n",
        "This is the code used in [this video](https://youtu.be/Kpm8rEywBDQ) where I try to explore why resumes might be getting (auto)rejected, and, in the process explore resume creation under different effort conditions.\n",
        "\n",
        "One treatment was an AI-assisted resume - this thing.\n",
        "\n",
        "In this notebook, I attempt to make a modularized resume generator with the assistance on AI. Given a job posting and a pre-existing resume, I will create several blocks of code that will generate or assist in the generation of:\n",
        "\n",
        "- Contact Information\n",
        "- Objective\n",
        "- Work Experience\n",
        "- Skills & Interests\n",
        "\n",
        "This notebook is uses forms to make it a little less scary for people who are code averse (and also because I didn't comment my code so it's pretty hard to parse lmao). Anyway, if you do want to see the code just click **show code**. The bulk of the stuff is in the helper function cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWrNJAgi7-yr",
        "cellView": "form"
      },
      "source": [
        "#@title Run Once at the Start\n",
        "#@markdown Installs and imports the packages necessary for this bad boy.\n",
        "#@markdown\n",
        "#@markdown Provide the path to a folder containing your resume and job application in CSV format.\n",
        "#@markdown\n",
        "#@markdown Examples here for [resume](https://docs.google.com/spreadsheets/d/1TnO_vdqVnmw1QKXLppENw58n6HXbfSuVmsOckUv2Jbg/edit?usp=sharing) and [job posting](https://docs.google.com/spreadsheets/d/1SDFtY45m3m5C2r9wokx8eNvv1oH-5pJgZuUCvJ2YY7w/edit?usp=sharing).\n",
        "#@markdown Because I don't want to spend time data cleaning, ur gonna need to format your resume and posting like the examples provided above otherwise this isn't gonna work lmao.\n",
        "#@markdown\n",
        "#@markdown Include the .csv at the end of the filenames. If the document isn't a .csv, buddy, u gotta go make it a .csv. This system simply is not robust.\n",
        "\n",
        "PATH = '' #@param {type:\"string\"}\n",
        "\n",
        "!pip install pysummarization\n",
        "!pip install pyinflect\n",
        "!pip install fuzzywuzzy\n",
        "\n",
        "import pyinflect\n",
        "from fuzzywuzzy import process, fuzz\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "from pysummarization.nlpbase.auto_abstractor import AutoAbstractor\n",
        "from pysummarization.tokenizabledoc.simple_tokenizer import SimpleTokenizer\n",
        "from pysummarization.abstractabledoc.top_n_rank_abstractor import TopNRankAbstractor\n",
        "\n",
        "import pandas as pd\n",
        "import math\n",
        "\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "import spacy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "\n",
        "from spacy.matcher import Matcher\n",
        "from spacy.tokens import Span\n",
        "from spacy import displacy\n",
        "from spacy.util import filter_spans\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "pd.set_option('display.max_colwidth', 200)\n",
        "\n",
        "path = PATH\n",
        "posting_filename = ''#@param{type:\"string\"}\n",
        "resume_filename = ''#@param{type:\"string\"}\n",
        "\n",
        "posting = pd.read_csv(path + '/' + posting_filename)\n",
        "resume = pd.read_csv(path + '/' + resume_filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eF3qc9YCacn8",
        "cellView": "form"
      },
      "source": [
        "#@title Helper Functions\n",
        "#@markdown This block contains helper functions. Don't judge my documentation. If you find that the text isn't being processed correctly with your examples, it's probably a problem with some of the functions in here. These bad boys are *not robust* lmao.\n",
        "\n",
        "# ---------------------------------------------------\n",
        "# Career Summary\n",
        "# ---------------------------------------------------\n",
        "\n",
        "def get_title(text):\n",
        "  \"\"\"\n",
        "  Given a title from a job posting, retain the key noun components to avoid copy-paste.\n",
        "  \"\"\"\n",
        "  pattern=[{'POS': 'NOUN', 'OP': '+'}]\n",
        "\n",
        "  # instantiate a Matcher instance\n",
        "  matcher = Matcher(nlp.vocab)\n",
        "\n",
        "  # Add pattern to matcher\n",
        "  matcher.add(\"verb-phrases\", None, pattern)\n",
        "\n",
        "  # apply text-preprocessing\n",
        "  text = prepro(text)\n",
        "\n",
        "  # create spacy object\n",
        "  doc = nlp(text)\n",
        "\n",
        "  # call the matcher to find matches\n",
        "  matches = matcher(doc)\n",
        "\n",
        "  spans = [doc[start:end] for _, start, end in matches]\n",
        "\n",
        "  filtered_spans = filter_spans(spans)\n",
        "\n",
        "  return str(filtered_spans[0])\n",
        "\n",
        "def summarize_posting(text):\n",
        "  \"\"\"\n",
        "  Given a paragraph string, return 1/2 of the most important sentences\n",
        "  as a form of text summary.\n",
        "  \"\"\"\n",
        "  # Object of automatic summarization.\n",
        "  auto_abstractor = AutoAbstractor()\n",
        "  # Set tokenizer.\n",
        "  auto_abstractor.tokenizable_doc = SimpleTokenizer()\n",
        "  # Set delimiter for making a list of sentence.\n",
        "  auto_abstractor.delimiter_list = [\".\", \"\\n\", \"?\"]\n",
        "  # Object of abstracting and filtering document.\n",
        "  abstractable_doc = TopNRankAbstractor()\n",
        "  # Summarize document.\n",
        "  result_dict = auto_abstractor.summarize(text, abstractable_doc)\n",
        "  scores = result_dict['scoring_data']\n",
        "  scores.sort(key = lambda x: x[1], reverse = True)\n",
        "\n",
        "  # return 1/3 of the most important sentences as a summary.\n",
        "  summary = []\n",
        "  max = math.ceil(len(scores)/2)\n",
        "  while len(summary) < max:\n",
        "    i = scores[0][0]\n",
        "    summary.append(result_dict['summarize_result'][i])\n",
        "    scores.pop(0)\n",
        "\n",
        "  return '\\n'.join(summary).strip()\n",
        "\n",
        "def gen_objective(job_title, job_summary):\n",
        "  \"\"\"\n",
        "  Prompt input for objective. Return a final objective.\n",
        "  \"\"\"\n",
        "  objective = 'I am a young professional and I am ready to be your {} because'.format(job_title)\n",
        "  addon = input('Job description summary:\\n\"{}\"\\n\\nYour current objective is:\\n{}\\n\\nWhy are you a good fit for this job?: \\n> '.format(job_summary, objective))\n",
        "  if addon == '':\n",
        "    objective = objective + '!'\n",
        "  else:\n",
        "    objective = objective + ' ' + addon\n",
        "  print()\n",
        "  print('Your new objective is:\\n{}'.format(objective))\n",
        "  return(objective)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------\n",
        "# Work Experience\n",
        "# ---------------------------------------------------\n",
        "\n",
        "def prepro(t, punc = False):\n",
        "  \"\"\"\n",
        "  Pre-process any strings. Doesn't delete punctuation because that causes some problems with rule-based matching.\n",
        "  We hate robust code.\n",
        "  \"\"\"\n",
        "  # lower case\n",
        "  t = t.lower()\n",
        "  # remove numbers\n",
        "  t = re.sub(r'\\d+', '', t)\n",
        "  # remove punctuation\n",
        "  if punc:\n",
        "    t = t.translate(str.maketrans('', '', string.punctuation))\n",
        "  # remove white space\n",
        "  t = t.strip()\n",
        "  return t\n",
        "\n",
        "def make_past(text):\n",
        "  \"\"\"\n",
        "  Given a string, if there are verbs present, return the string with verbs converted to past tense (if applicable)\n",
        "  \"\"\"\n",
        "\n",
        "  doc_dep = nlp(text)\n",
        "  for i in range(len(doc_dep)):\n",
        "      token = doc_dep[i]\n",
        "      if token.tag_ in ['VBP', 'VBZ', 'VBG', 'VB', \"VBP\"]:\n",
        "          # print(token.text, token.lemma_, token.pos_, token.tag_)\n",
        "          text = text.replace(token.text, token._.inflect(\"VBD\"))\n",
        "  return text\n",
        "\n",
        "def make_present(text):\n",
        "  \"\"\"\n",
        "  Given a string, if there are verbs present, return the string with verbs converted to present tense (if applicable)\n",
        "  \"\"\"\n",
        "\n",
        "  doc_dep = nlp(text)\n",
        "  for i in range(len(doc_dep)):\n",
        "      token = doc_dep[i]\n",
        "      if token.tag_ in ['VBP', 'VBZ', 'VBG', 'VB', \"VBP\"]:\n",
        "          # print(token.text, token.lemma_, token.pos_, token.tag_)\n",
        "          text = text.replace(token.text, token._.inflect(\"VBZ\"))\n",
        "  return text\n",
        "\n",
        "def get_actions(responsibilities):\n",
        "  \"\"\"\n",
        "  Given a list of responsibilities from a job posting, return a summary of those responsibilities as action items\n",
        "  \"\"\"\n",
        "  actions = []\n",
        "\n",
        "  pattern=[{'POS': 'VERB', 'OP': '+'},\n",
        "           {'POS': 'ADJ', 'OP': '*'},\n",
        "           {'POS': 'ADP', 'OP': '*'},\n",
        "           {'POS': 'NOUN', 'OP': '+'}]\n",
        "\n",
        "  # instantiate a Matcher instance\n",
        "  matcher = Matcher(nlp.vocab)\n",
        "\n",
        "  # Add pattern to matcher\n",
        "  matcher.add(\"verb-phrases\", None, pattern)\n",
        "\n",
        "  for r in responsibilities:\n",
        "    # apply text-preprocessing\n",
        "    #r = prepro(r)\n",
        "\n",
        "    # create spacy object\n",
        "    doc = nlp(r)\n",
        "\n",
        "    # call the matcher to find matches\n",
        "    matches = matcher(doc)\n",
        "\n",
        "    spans = [doc[start:end] for _, start, end in matches]\n",
        "\n",
        "    filtered_spans = filter_spans(spans)\n",
        "\n",
        "    for span in filtered_spans:\n",
        "      t = make_past(str(span))\n",
        "      t = t[0].upper() + t[1:]\n",
        "      if t[-1] != '.':\n",
        "        t = t+'.'\n",
        "      actions.append(t)\n",
        "\n",
        "  return actions\n",
        "\n",
        "def match_actions(job_actions, strOptions):\n",
        "  \"\"\"\n",
        "  Given a set of strings detailing job actions, find each elements best match in a second list of past actions.\n",
        "  Return a list of tuples detailing each job action, its best match, and the corresponding score.\n",
        "  \"\"\"\n",
        "  matches = []\n",
        "  for action in job_actions:\n",
        "    str2Match = action\n",
        "    best_option = strOptions[0]\n",
        "    best_score = fuzz.token_sort_ratio(str2Match, strOptions[0])\n",
        "    # manually comparing using fuzz.token_sort_ratio over process.extractOne because the latter's choices are kinda whack.\n",
        "    for option in strOptions:\n",
        "      option_score = fuzz.token_sort_ratio(str2Match, option)\n",
        "      if option_score >= best_score:\n",
        "        best_score = option_score\n",
        "        best_option = option\n",
        "    matches.append((str2Match, best_option, best_score))\n",
        "    # for applications and resumes with closer matches, it would be interesting to set a cap on similarity to avoid repeat entries\n",
        "\n",
        "  return matches\n",
        "\n",
        "def match_action_to_job(job_actions, past_actions, resume):\n",
        "  \"\"\"\n",
        "  This function is pretty disgusting ngl. It is heavily reliant on messy formatting restricted to this pass at code.\n",
        "  Anyway, it takes\n",
        "  \"\"\"\n",
        "  exp_in_job = []\n",
        "  action_matches = pd.DataFrame(matches, columns = ['job_action', 'best_match', 'score'])\n",
        "\n",
        "  for e in action_matches.values:\n",
        "    action = e[0]\n",
        "    match = e[1]\n",
        "    match_i = resume[resume['my_actions']==match].index.values\n",
        "    exp_in_job.append((action, resume.loc[match_i, 'my_titles'].values[0]))\n",
        "\n",
        "  for e in resume.values:\n",
        "    action = e[2]\n",
        "    job = e[1]\n",
        "    exp_in_job.append((action, job))\n",
        "\n",
        "  exp_in_job = pd.DataFrame(exp_in_job, columns = ['action', 'title'])\n",
        "\n",
        "  titles = []\n",
        "  action_job_matches = {}\n",
        "\n",
        "  for i in range(len(exp_in_job)):\n",
        "    t = exp_in_job.loc[i, 'title']\n",
        "    a = exp_in_job.loc[i, 'action']\n",
        "    if t not in titles:\n",
        "      titles.append(t)\n",
        "      action_job_matches[t] = []\n",
        "    action_job_matches[t].append(a)\n",
        "  return action_job_matches\n",
        "\n",
        "# --------------------------------------\n",
        "# Skills & Interests\n",
        "# --------------------------------------\n",
        "\n",
        "def extract_interests(sq_list):\n",
        "  \"\"\"\n",
        "  Given a list of skills and qualifications on a job posting, extract a set of interests.\n",
        "  \"\"\"\n",
        "  interests = []\n",
        "  # may start with a noun, absolutely cannot have a verb, must end with a noun\n",
        "  pattern = [{'POS': 'NOUN', 'OP': '*'},\n",
        "            {'POS': 'VERB', 'OP': '!'},\n",
        "            {'POS': 'NOUN', 'OP': '+'}]\n",
        "\n",
        "  # instantiate a Matcher instance\n",
        "  matcher = Matcher(nlp.vocab)\n",
        "\n",
        "  # Add pattern to matcher\n",
        "  matcher.add(\"verb-phrases\", None, pattern)\n",
        "\n",
        "  for e in sq_list:\n",
        "    doc = nlp(prepro(e, punc = True))\n",
        "    matches = matcher(doc)\n",
        "    spans = [doc[start:end] for _, start, end in matches]\n",
        "    for e in filter_spans(spans):\n",
        "      for chunk in e.noun_chunks:\n",
        "        interests.append(chunk.text)\n",
        "\n",
        "  return interests\n",
        "\n",
        "def extract_skills(sq_list):\n",
        "  \"\"\"\n",
        "  Given a list of skills and qualifications on a job posting, extract a set of interests.\n",
        "  \"\"\"\n",
        "  output = []\n",
        "  # may start with a noun, absolutely cannot have a verb, must end with a noun\n",
        "  pattern = [{'POS': 'VERB', 'OP': '+'},\n",
        "            {'POS': 'ADP', 'OP': '*'},\n",
        "            {'POS': 'ADJ', 'OP': '*'},\n",
        "            {'POS': 'NOUN', 'OP': '+'}]\n",
        "\n",
        "  # instantiate a Matcher instance\n",
        "  matcher = Matcher(nlp.vocab)\n",
        "\n",
        "  # Add pattern to matcher\n",
        "  matcher.add(\"verb-phrases\", None, pattern)\n",
        "\n",
        "  for e in sq_list:\n",
        "    doc = nlp(prepro(e, punc = True))\n",
        "    matches = matcher(doc)\n",
        "    spans = [doc[start:end] for _, start, end in matches]\n",
        "    for e in filter_spans(spans):\n",
        "      output.append(make_present(str(e)))\n",
        "  return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ht9z-jbsLVGZ",
        "cellView": "form"
      },
      "source": [
        "#@title Personal Information\n",
        "#@markdown This information isn't stored in any way but if you don't feel comfortable putting stuff in, that's fine. It's literally just for the string export but you can keep add it urself in a local file.\n",
        "name = ''#@param{type:'string'}\n",
        "number = ''#@param{type:'string'}\n",
        "email = ''#@param{type:'string'}\n",
        "portfolio = ''#@param{type:'string'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOxLzvben2Xy",
        "cellView": "form"
      },
      "source": [
        "#@title Objective / Mission Statement\n",
        "#@markdown Run this cell to generate a resume objective. It'll prompt you to input an objective. You will need to use your brain for this one. Or just leave it blank because apparently objectives on resumes are outdated (they are good for padding with buzzwords from the listing though).\n",
        "#@markdown\n",
        "#@markdown I use a truly garbage system to print out 30% of the most important sentences in the job_summary - a summary of a summary basically.\n",
        "#@markdown\n",
        "job_title = get_title(posting.job_title[0])\n",
        "job_summary = summarize_posting(posting.job_summary[0])\n",
        "objective = gen_objective(job_title, job_summary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcnwXacdcYg-",
        "cellView": "form"
      },
      "source": [
        "#@title Work Experience\n",
        "#@markdown Here's what happens when you run this cell:\n",
        "#@markdown * Takes the responsibilities listed in the job posting and converts them into past tense\n",
        "#@markdown * Compares those phrases to the work experience in your resume using fuzzy matching\n",
        "#@markdown * Attributes those phrases to the job title with the highest matching experience\n",
        "\n",
        "# convert job responsibilities to past experience phrases\n",
        "job_actions = get_actions(posting.job_responsibilities)\n",
        "\n",
        "# match experiences to resume experiences and jobs\n",
        "past_actions = resume.my_actions.values\n",
        "matches = match_actions(job_actions, past_actions)\n",
        "action_job_matches = match_action_to_job(job_actions, past_actions, resume)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoEluLiKTrGI",
        "cellView": "form"
      },
      "source": [
        "#@title Skills & Interests\n",
        "#@markdown Here's what happens when you run this cell:\n",
        "#@markdown * Extracts noun clusters from qualifications and responsibilities from the job posting\n",
        "#@markdown * Sorts them into interests and skills depending on the format - there's a pretty high chance at repetition between these lists so watch out.\n",
        "interests = extract_interests(posting.job_skills)\n",
        "skills = extract_skills(posting.job_skills)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLNqLDFxRllo",
        "cellView": "form"
      },
      "source": [
        "#@title Write Resume\n",
        "#@markdown If you've successfully run all the cells above, this cell should run and end with a prompt to download a .txt of a lightly formatted resume.\n",
        "#@markdown\n",
        "#@markdown **It will not be a good resume.** It will require a lot of human input to cull the noise and check for relevancy and accuracy to your actual experiences.\n",
        "#@markdown\n",
        "#@markdown In an ideal world, this new resume will serve as a starting point for resume customizations and will ultimately rely on you to fix it up and make it legible for humans.\n",
        "#@markdown\n",
        "#@markdown In our actual world, it's the messy result of a 5 hours coding session featured in [this video](https://youtu.be/Kpm8rEywBDQ).\n",
        "# initialize. not really necessary but why not.\n",
        "new_resume = ''\n",
        "\n",
        "# header information\n",
        "new_resume += '{}\\n{} | {} | {}'.format(name, number, email, portfolio)\n",
        "\n",
        "# objective header\n",
        "new_resume += '\\n\\nOBJECTIVE\\n'+('─' * 50)\n",
        "\n",
        "# add objective\n",
        "new_resume += '\\n{}\\n\\n'.format(objective)\n",
        "\n",
        "# work experience header\n",
        "new_resume += 'WORK EXPERIENCE\\n'+('─' * 50) +'\\n'\n",
        "\n",
        "# add jobs and experiences\n",
        "for t in action_job_matches.keys():\n",
        "  new_resume += \"{}\\n\".format(t)\n",
        "  for a in action_job_matches[t]:\n",
        "    new_resume += '- {}\\n'.format(a)\n",
        "  new_resume += '\\n'\n",
        "\n",
        "# skills experience header\n",
        "new_resume += 'SKILLS\\n'+('─' * 50) +'\\n'\n",
        "new_resume += ', '.join(skills) + '\\n\\n'\n",
        "\n",
        "# interests experience header\n",
        "new_resume += 'INTERESTS\\n'+('─' * 50) +'\\n'\n",
        "new_resume += ', '.join(interests)\n",
        "\n",
        "# write to string and prompt download as a .txt\n",
        "textfile = open('{} Resume.txt'.format(job_title), 'w')\n",
        "textfile.write(new_resume)\n",
        "textfile.close()\n",
        "files.download('{} Resume.txt'.format(job_title))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}